{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from table_extractor import table_extractor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.037037037037038\n",
      "Skipping small bbox\n",
      "3.3884297520661155\n",
      "17.037037037037038\n",
      "Skipping small bbox\n",
      "2.7851239669421486\n",
      "3.8095238095238093\n",
      "Area: (254, 101, 375, 511) of page no 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Oct 09, 2024 5:37:27 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'ArialMT'\n",
      "Oct 09, 2024 5:37:27 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'LiberationSans' for 'Arial-BoldMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                 Principal Unnamed: 1 Unnamed: 2   CUSIP\n",
      "0    (1)Year Amount Coupon      Yield      Price  021807\n",
      "1  12/1/2019 $5,000 3.000%     2.050%   101.380%     KL5\n",
      "2  12/1/2020 $5,000 3.000%     2.200%   101.922%     KM3\n",
      "3  12/1/2021 $5,000 3.000%     2.300%   102.330%     KN1\n",
      "4  12/1/2022 $5,000 3.000%     2.450%   102.321%     KP6\n",
      "5  12/1/2023 $5,000 3.000%     2.600%   102.031%     KQ4\n",
      "6  12/1/2024 $5,000 3.000%     2.700%   101.773%     KR2]\n"
     ]
    }
   ],
   "source": [
    "from table_extractor import table_extractor\n",
    "\n",
    "\n",
    "id = \"ES1156021-ES903925-ES1305165\"\n",
    "PDF_FILE = f\"/OTTO-Project/EMMA_OFFICIAL_STATEMENT_TEMP_PDF/{id}.pdf\"\n",
    "\n",
    "extractor = table_extractor.TableExtractor(PDF_FILE)\n",
    "\n",
    "df_map = extractor.extract_tables(page_range=[1])\n",
    "\n",
    "with open(f'/home/factentry/otto_ml/src/antlr/mondal/pdf_table_extractor/{id}.csv', 'w+') as f:\n",
    "    print(f'\"File Name\",\"{PDF_FILE}\"', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = duckdb.connect(\"/home/factentry/otto_ml/src/antlr/mondal/pdf_table_extractor/extraction.db\")\n",
    "cursor.sql(\"DROP TABLE extraction\")\n",
    "cursor.sql(\"CREATE TABLE IF NOT EXISTS extraction (file_id VARCHAR)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_inserter(cursor, df, file_id):\n",
    "    for header in df.columns.values:\n",
    "        cursor.execute(f\"\"\"AlTER TABLE extraction ADD COLUMN IF NOT EXISTS \"{header.strip()}\" VARCHAR\"\"\")\n",
    "    \n",
    "    cursor.execute(f\"INSERT INTO extraction BY NAME SELECT '{file_id.strip()}' as file_id, * FROM df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_header_weight(headers):\n",
    "    ALLOWED_HEADERS = [\n",
    "        \"maturity\",\n",
    "        \"redemption\",\n",
    "        \"date\",\n",
    "        \"year\",\n",
    "        \"due\",\n",
    "        \"principal\",\n",
    "        \"amount\",\n",
    "        \"price\",\n",
    "        \"interest\",\n",
    "        \"rate\",\n",
    "        \"yield\",\n",
    "        \"cusip\"\n",
    "    ]\n",
    "\n",
    "    unmatched = []\n",
    "    match_count = 0\n",
    "\n",
    "    for header in headers:\n",
    "        if any(kw in header.lower().strip() for kw in ALLOWED_HEADERS):\n",
    "            match_count += 1\n",
    "        else:\n",
    "            unmatched.append(header)\n",
    "    \n",
    "    return len(headers) == match_count, unmatched\n",
    "\n",
    "\n",
    "\n",
    "def standardize_table(df: pd.core.frame.DataFrame, file_id):\n",
    "    all_headers_allowed, unmatched_headers = validate_header_weight(df.columns.values)\n",
    "    if all_headers_allowed:\n",
    "        table_inserter(cursor, df, file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.sql(\"COPY (SELECT * FROM extraction) TO '/home/factentry/otto_ml/src/antlr/mondal/pdf_table_extractor/extraction.parquet' (FORMAT 'parquet');\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIGIT', 'DIGIT', 'DIGIT', 'DIGIT']\n"
     ]
    }
   ],
   "source": [
    "def infer_text_pattern(str):\n",
    "    str_pattern=[]\n",
    "    literal_identifier_map = {\n",
    "        \"-\": \"DASH\",\n",
    "        \"$\": \"DOLLAR\",\n",
    "        \"%\": \"PERCENT\",\n",
    "        \",\": \"COMMA\",\n",
    "        \".\": \"DOT\",\n",
    "    }\n",
    "    for c in str:\n",
    "        if c.isdigit():\n",
    "            str_pattern.append(\"DIGIT\")\n",
    "        elif c.isspace():\n",
    "            str_pattern.append(\"SPACE\")\n",
    "        elif c.isalpha():\n",
    "            str_pattern.append(\"CHAR\")\n",
    "        elif c in literal_identifier_map:\n",
    "            str_pattern.append(literal_identifier_map[c])\n",
    "        else:\n",
    "            str_pattern.append(\"SPLCHAR\")\n",
    "    return str_pattern\n",
    "\n",
    "\n",
    "print(infer_text_pattern(\"2024\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034-07-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "dt = parser.parse(\"7-1-2034\")\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def can_be_currency(str):\n",
    "    if str[0] == \"$\":\n",
    "        return True\n",
    "    elif re.sub(r'[,\\.\\*]', '', str).isnumeric() and len(str) > 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def try_parse_date(str):\n",
    "    if len(str) < 4:\n",
    "        return None\n",
    "    try:\n",
    "        return parser.parse(str.replace(' ', ''))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def can_be_rate(str):\n",
    "    return re.sub(r'[%\\*\\.]', '', str).isnumeric()\n",
    "\n",
    "def can_be_cusip_part(str):\n",
    "    str = re.sub(r'[\\@\\* ]', '', str)\n",
    "    return len(str) == 3 and str[-1].isdigit()\n",
    "\n",
    "def can_be_cusip(str):\n",
    "    str = re.sub(r'[\\@\\* ]', '', str)\n",
    "    return len(str) == 9 and can_be_cusip_part(str[-3:])\n",
    "\n",
    "def infer_type(str):\n",
    "    str = str.encode('ascii', errors='ignore').decode() # Filter out non-ascii characters for now\n",
    "    if not str or str.lower() == \"nan\":\n",
    "        return \"NAN\"\n",
    "    elif str.replace('.', '', 1).replace('-', '', 1).isnumeric():\n",
    "        return \"NUMBER\"\n",
    "    elif str.isalpha():\n",
    "        return \"STRING\"\n",
    "    elif str.isascii():\n",
    "        if len(str.translate(str.maketrans('', '', string.punctuation + string.whitespace))) > 15:\n",
    "            return \"SENTENCE\"\n",
    "        elif can_be_currency(str):\n",
    "            return \"CURRENCY\"\n",
    "        elif try_parse_date(str):\n",
    "            return \"DATE\"\n",
    "        elif can_be_rate(str):\n",
    "            return \"RATE\"\n",
    "        elif can_be_cusip(str):\n",
    "            return \"CUSIP\"\n",
    "        elif can_be_cusip_part(str):\n",
    "            return \"CUSIP_PART\"\n",
    "        \n",
    "    return \" \".join(infer_text_pattern(str))\n",
    "\n",
    "\n",
    "print(infer_type(\"2024\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_DTYPE_MAP = {\n",
    "    \"and\": [\"ANY\"],\n",
    "    \"cusip\": [\"CUSIP\", \"CUSIP_PART\"],\n",
    "    \"date\": [\"DATE\", \"SENTENCE\"],\n",
    "    \"maturity\": [\"DATE\", \"SENTENCE\"],\n",
    "    \"rate\": [\"RATE\", \"NUMBER\"],\n",
    "    \"yield\": [\"RATE\", \"NUMBER\"],\n",
    "    \"amount\": [\"CURRENCY\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def clean_df(df):\n",
    "    is_multiline_header = False\n",
    "    # Scan for data type column wise\n",
    "    for column in df.columns:\n",
    "        column_value_dtypes = [(k, len(list(g))) for k, g in itertools.groupby([infer_type(str(x)) for x in df[column]])]\n",
    "        if len(column_value_dtypes) == 2:\n",
    "            if column_value_dtypes[0][1] <= 2:\n",
    "                is_multiline_header = True\n",
    "            else:\n",
    "                is_multiline_header = False\n",
    "    \n",
    "    all_null_columns = df.isnull().values.all(axis=0)\n",
    "\n",
    "    all_nan_ranges = []\n",
    "    consecutive_nan_range = (None, None)\n",
    "    for idx, col in enumerate(df.columns):\n",
    "        if col.lower() == \"nan\" or \"unnamed\" in col.lower():\n",
    "            next_col_name = df.columns[min(idx + 1, len(df.columns) - 1)].lower()\n",
    "            if next_col_name != \"nan\" or \"unnamed\" not in next_col_name:\n",
    "                if not consecutive_nan_range[0]:\n",
    "                    consecutive_nan_range = (idx, None)\n",
    "            else:\n",
    "                consecutive_nan_range = (consecutive_nan_range[0], idx)\n",
    "                all_nan_ranges.append(consecutive_nan_range)\n",
    "                consecutive_nan_range = (None, None)\n",
    "    \n",
    "\n",
    "        \n",
    "    if is_multiline_header:\n",
    "        current_headers = [\"\" if header.lower().startswith(\"unnamed\") else header for header in df.columns.tolist()]\n",
    "        first_row_values = df.iloc[0].tolist()\n",
    "        new_headers = [f\"{header} {value}\".strip() for header, value in zip(current_headers, first_row_values)]\n",
    "        df.columns = new_headers\n",
    "        df = df.drop(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test main impl\n",
    "\n",
    "import muni_table_standardiser as standardiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "!!!! Merge success !!!!\n",
      "WARN: Found column type of ANY. Considering as freezed column\n",
      "    nan   nan              cusip   nan      maturity      amount  \\\n",
      "1  None  None  3 4 1 5 3 Q E D 4  None  June 1, 2018  $6,845,000   \n",
      "2  None  None  3 4 1 5 3 Q E E 2  None  June 1, 2019   7,190,000   \n",
      "3  None  None  3 4 1 5 3 Q E F 9  None  June 1, 2020   7,550,000   \n",
      "4  None  None  3 4 1 5 3 Q E G 7  None  June 1, 2021   7,925,000   \n",
      "5  None  None  3 4 1 5 3 Q E H 5  None  June 1, 2022   8,325,000   \n",
      "\n",
      "  interest_rate  yield redemption_date  \n",
      "1         5.00%  0.62%               -  \n",
      "2          5.00   0.67               -  \n",
      "3          5.00   0.78               -  \n",
      "4          5.00   0.93               -  \n",
      "5          5.00   1.08               -  \n"
     ]
    }
   ],
   "source": [
    "for page_no, dfs in df_map.items():\n",
    "    for df in dfs:\n",
    "        if df.shape[1] == 1:\n",
    "                continue\n",
    "        if not df.shape[0]:\n",
    "            continue\n",
    "\n",
    "        if len(df.iloc[0]) < 2:\n",
    "            continue\n",
    "\n",
    "        row_count = df.shape[0]\n",
    "\n",
    "        is_line = False\n",
    "\n",
    "        for ri in range(row_count):\n",
    "            words = str(df.iloc[ri][0]).split()\n",
    "            avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "            if len(words) > 10 and avg_word_length > 5:\n",
    "                is_line = True\n",
    "                continue\n",
    "        \n",
    "        if is_line:\n",
    "            continue\n",
    "        with open(f'/home/factentry/otto_ml/src/antlr/mondal/pdf_table_extractor/{id}.csv', 'a') as f:\n",
    "            print(f'\\n', file=f)\n",
    "\n",
    "            df = standardiser.clean_df(df)\n",
    "            header_list = list(df.columns.values)\n",
    "            header_list = [\"\" if header.lower().startswith(\"unnamed\") or header.lower().startswith(\"nan\") else header for header in header_list]\n",
    "            # standardize_table(df, id)\n",
    "\n",
    "            # Standardize headers\n",
    "            # current_headers = [\"\" if header.lower().startswith(\"unnamed\") else header for header in df.columns.tolist()]\n",
    "            # first_row_values = df.iloc[0].tolist()\n",
    "            # new_headers = [f\"{header} {value}\".strip() for header, value in zip(current_headers, first_row_values)]\n",
    "            # df.columns = new_headers\n",
    "            # df = df.drop(0)\n",
    "\n",
    "            print(df.head())\n",
    "            df.to_csv(f, index=False, header=header_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "len(\"U.S. Bank Trust National Association, New York, New York.\".translate(str.maketrans('', '', string.punctuation + string.whitespace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE\n"
     ]
    }
   ],
   "source": [
    "print(infer_type(\"U.S. Bank Trust National Association, New York, New York.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str_list = [\"3\",   \"4\",    \"1 5 3 Q E D\",   \"4\"  , \"NaN\"  ,\"June 1, 2018\",       \"$6,845,000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '4', '1 5 3 Q E D', '4', 'NaN', 'June 1, 2018', '$6,845,000']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '4', '1 5 3 Q E D', '4', '', 'June 1, 2018', '$6,845,000']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x if x.lower().strip() not in [\"nan\", \"none\", \"null\"] else \"\", test_str_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str_list = list(map(lambda x: x if x.lower().strip() not in [\"nan\", \"none\", \"null\"] else \"\", test_str_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "infr_type_list = [infer_type(e) for e in test_str_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_expected_type = [None, None, \"CUSIP\", None, None, \"DATE\", \"CURRENCY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "freezed_columns = []\n",
    "missmatch_cols = {}\n",
    "\n",
    "for col_idx, (exp_type, inferred_type) in enumerate(zip(test_expected_type, infr_type_list)):\n",
    "    if not exp_type:\n",
    "        missmatch_cols[col_idx] = \"U\"\n",
    "    elif exp_type == inferred_type:\n",
    "        freezed_columns.append(col_idx)\n",
    "    elif exp_type != inferred_type:\n",
    "        missmatch_cols[col_idx] = \"M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freezed_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'U', 1: 'U', 2: 'M', 3: 'U', 4: 'U'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missmatch_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '1 5 3 Q E D', '4', '', 'June 1, 2018', '$6,845,000']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: (0, 1)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({2: '3 4 1 5 3 Q E D 4'},)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = -1\n",
    "end_idx = -1\n",
    "last_mismatch_idx = -1\n",
    "merged_column_contents = {}\n",
    "merge_region = {}\n",
    "for col_idx, miss_type in missmatch_cols.items():\n",
    "    if miss_type == \"U\":\n",
    "        if start_idx == -1:\n",
    "            start_idx = col_idx\n",
    "        if end_idx == -1:\n",
    "            if missmatch_cols[min(col_idx + 1, len(missmatch_cols) - 1)] == \"M\":\n",
    "                end_idx = col_idx\n",
    "    elif miss_type == \"M\":\n",
    "        merged_column_contents[col_idx] = \" \".join(test_str_list[start_idx:end_idx+1] + [test_str_list[col_idx]])\n",
    "        merge_region[col_idx] = (start_idx, end_idx)\n",
    "        start_idx = -1\n",
    "        end_idx = -1\n",
    "        last_mismatch_idx = col_idx\n",
    "\n",
    "print(merge_region)\n",
    "\n",
    "# No match found on left side. Merge <- direction\n",
    "if end_idx == -1:\n",
    "    merged_column_contents[last_mismatch_idx] = (merged_column_contents[last_mismatch_idx] + \" \" + \" \".join(test_str_list[start_idx:list(missmatch_cols.keys())[-1] + 1])).strip()\n",
    "    merge_region[last_mismatch_idx] = (merge_region[last_mismatch_idx][0], list(missmatch_cols.keys())[-1])\n",
    "\n",
    "merged_column_contents, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! Merge success !!!!\n"
     ]
    }
   ],
   "source": [
    "for col_idx, content in merged_column_contents.items():\n",
    "    if infer_type(content) == test_expected_type[col_idx]:\n",
    "        print(\"!!!! Merge success !!!!\")\n",
    "        # do actual merge on df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(psutil.cpu_count() // 100) * 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=541014310912, available=306733891584, percent=43.3, used=231291240448, free=308231770112, active=112231190528, inactive=116548583424, buffers=78376960, cached=1412923392, shared=413696, slab=1241886720)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215721493260"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(psutil.virtual_memory().free // 100) * 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mongo_client = MongoClient('mongodb://192.168.1.245:27017/?ssl=false')\n",
    "mongo_db = mongo_client['otto_ml']\n",
    "mongo_collection = mongo_db['muni_tables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['192.168.1.245:27017'], document_class=dict, tz_aware=False, connect=True, tls=False), 'otto_ml'), 'muni_tables')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongo_collection.insert_many(df.to_dict('records'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
